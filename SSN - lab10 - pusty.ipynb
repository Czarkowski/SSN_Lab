{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814704ca-3c42-4491-a8b9-6a14ad2c71b1",
   "metadata": {},
   "source": [
    "## Sztuczne sieci neuronowe - laboratorium 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232faca3-7e9f-4c77-9a78-11f7b94fc537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e71cfa-7e7d-4193-9208-d449cbb12c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# sprawdzenie, czy GPU jest widoczne\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6383c1b4-ff57-4721-86f2-8ec0ff0e0790",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "Dzisiejsze zajęcia będą dotyczyły zagadnienia **transfer learning** - trenowania modeli polegającego na wykorzystaniu architektury i zestawu wag wytrenowanych wcześniej (np. przez kogoś innego - najczęściej badaczy z największych firm, na dużym zbiorze danych), celem wykorzystania \"wiedzy\" zgromadzonej w już wytrenowanym modelu i przeniesienia jej (stąd \"transfer\") do innego, zwykle węższego problemu (np. poprzez dotrenowanie na znacznie mniejszym zbiorze danych).\n",
    "\n",
    "Fazy te nazywają się odpowiednio **pre-training** (tzw. modele pretrenowane, *pretrained models*) i **fine-tuning**.\n",
    "\n",
    "Wiele z takich gotowych (pretrenowanych) modeli dostępnych jest w pakiecie `torchvision` - części PyTorcha związanej z przetwarzaniem obrazów."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab0b22-defd-4dc6-9ecc-c28f663d7a96",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "Uruchom poniższą komórkę, aby wypisać dostępne w `torchvision` modele. Nazwy modeli zaczynające się dużą literą oznaczają klasy implementujące poszczególne architektury sieci. Ich odpowiedniki pisane małymi literami to funkcje pozwalające zainicjalizować model (https://pytorch.org/vision/stable/models.html).\n",
    "\n",
    "Funkcje te mają argument `pretrained` - gdy podamy wartość `True`, inicjalizujemy model pretrenowanymi wagami (dla `False` - losowymi).\n",
    "\n",
    "Wczytaj po kolei wybrane modele (np. `resnet18`) do zmiennej. Sprawdź jej zawartość."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd40543c-050f-4418-95fe-bc20e9d2fa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlexNet',\n",
       " 'AlexNet_Weights',\n",
       " 'ConvNeXt',\n",
       " 'ConvNeXt_Base_Weights',\n",
       " 'ConvNeXt_Large_Weights',\n",
       " 'ConvNeXt_Small_Weights',\n",
       " 'ConvNeXt_Tiny_Weights',\n",
       " 'DenseNet',\n",
       " 'DenseNet121_Weights',\n",
       " 'DenseNet161_Weights',\n",
       " 'DenseNet169_Weights',\n",
       " 'DenseNet201_Weights',\n",
       " 'EfficientNet',\n",
       " 'EfficientNet_B0_Weights',\n",
       " 'EfficientNet_B1_Weights',\n",
       " 'EfficientNet_B2_Weights',\n",
       " 'EfficientNet_B3_Weights',\n",
       " 'EfficientNet_B4_Weights',\n",
       " 'EfficientNet_B5_Weights',\n",
       " 'EfficientNet_B6_Weights',\n",
       " 'EfficientNet_B7_Weights',\n",
       " 'EfficientNet_V2_L_Weights',\n",
       " 'EfficientNet_V2_M_Weights',\n",
       " 'EfficientNet_V2_S_Weights',\n",
       " 'GoogLeNet',\n",
       " 'GoogLeNetOutputs',\n",
       " 'GoogLeNet_Weights',\n",
       " 'Inception3',\n",
       " 'InceptionOutputs',\n",
       " 'Inception_V3_Weights',\n",
       " 'MNASNet',\n",
       " 'MNASNet0_5_Weights',\n",
       " 'MNASNet0_75_Weights',\n",
       " 'MNASNet1_0_Weights',\n",
       " 'MNASNet1_3_Weights',\n",
       " 'MaxVit',\n",
       " 'MaxVit_T_Weights',\n",
       " 'MobileNetV2',\n",
       " 'MobileNetV3',\n",
       " 'MobileNet_V2_Weights',\n",
       " 'MobileNet_V3_Large_Weights',\n",
       " 'MobileNet_V3_Small_Weights',\n",
       " 'RegNet',\n",
       " 'RegNet_X_16GF_Weights',\n",
       " 'RegNet_X_1_6GF_Weights',\n",
       " 'RegNet_X_32GF_Weights',\n",
       " 'RegNet_X_3_2GF_Weights',\n",
       " 'RegNet_X_400MF_Weights',\n",
       " 'RegNet_X_800MF_Weights',\n",
       " 'RegNet_X_8GF_Weights',\n",
       " 'RegNet_Y_128GF_Weights',\n",
       " 'RegNet_Y_16GF_Weights',\n",
       " 'RegNet_Y_1_6GF_Weights',\n",
       " 'RegNet_Y_32GF_Weights',\n",
       " 'RegNet_Y_3_2GF_Weights',\n",
       " 'RegNet_Y_400MF_Weights',\n",
       " 'RegNet_Y_800MF_Weights',\n",
       " 'RegNet_Y_8GF_Weights',\n",
       " 'ResNeXt101_32X8D_Weights',\n",
       " 'ResNeXt101_64X4D_Weights',\n",
       " 'ResNeXt50_32X4D_Weights',\n",
       " 'ResNet',\n",
       " 'ResNet101_Weights',\n",
       " 'ResNet152_Weights',\n",
       " 'ResNet18_Weights',\n",
       " 'ResNet34_Weights',\n",
       " 'ResNet50_Weights',\n",
       " 'ShuffleNetV2',\n",
       " 'ShuffleNet_V2_X0_5_Weights',\n",
       " 'ShuffleNet_V2_X1_0_Weights',\n",
       " 'ShuffleNet_V2_X1_5_Weights',\n",
       " 'ShuffleNet_V2_X2_0_Weights',\n",
       " 'SqueezeNet',\n",
       " 'SqueezeNet1_0_Weights',\n",
       " 'SqueezeNet1_1_Weights',\n",
       " 'SwinTransformer',\n",
       " 'Swin_B_Weights',\n",
       " 'Swin_S_Weights',\n",
       " 'Swin_T_Weights',\n",
       " 'Swin_V2_B_Weights',\n",
       " 'Swin_V2_S_Weights',\n",
       " 'Swin_V2_T_Weights',\n",
       " 'VGG',\n",
       " 'VGG11_BN_Weights',\n",
       " 'VGG11_Weights',\n",
       " 'VGG13_BN_Weights',\n",
       " 'VGG13_Weights',\n",
       " 'VGG16_BN_Weights',\n",
       " 'VGG16_Weights',\n",
       " 'VGG19_BN_Weights',\n",
       " 'VGG19_Weights',\n",
       " 'ViT_B_16_Weights',\n",
       " 'ViT_B_32_Weights',\n",
       " 'ViT_H_14_Weights',\n",
       " 'ViT_L_16_Weights',\n",
       " 'ViT_L_32_Weights',\n",
       " 'VisionTransformer',\n",
       " 'Weights',\n",
       " 'WeightsEnum',\n",
       " 'Wide_ResNet101_2_Weights',\n",
       " 'Wide_ResNet50_2_Weights',\n",
       " '_GoogLeNetOutputs',\n",
       " '_InceptionOutputs',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_api',\n",
       " '_meta',\n",
       " '_utils',\n",
       " 'alexnet',\n",
       " 'convnext',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'densenet',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'detection',\n",
       " 'efficientnet',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_v2_l',\n",
       " 'efficientnet_v2_m',\n",
       " 'efficientnet_v2_s',\n",
       " 'get_model',\n",
       " 'get_model_builder',\n",
       " 'get_model_weights',\n",
       " 'get_weight',\n",
       " 'googlenet',\n",
       " 'inception',\n",
       " 'inception_v3',\n",
       " 'list_models',\n",
       " 'maxvit',\n",
       " 'maxvit_t',\n",
       " 'mnasnet',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'mobilenetv2',\n",
       " 'mobilenetv3',\n",
       " 'optical_flow',\n",
       " 'quantization',\n",
       " 'regnet',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'resnext50_32x4d',\n",
       " 'segmentation',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'shufflenetv2',\n",
       " 'squeezenet',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'swin_b',\n",
       " 'swin_s',\n",
       " 'swin_t',\n",
       " 'swin_transformer',\n",
       " 'swin_v2_b',\n",
       " 'swin_v2_s',\n",
       " 'swin_v2_t',\n",
       " 'vgg',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'video',\n",
       " 'vision_transformer',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_h_14',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed0411d0-76bb-4481-98bb-60fbebe15401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cezar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\cezar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\cezar/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:02<00:00, 16.7MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "resnet18_model = models.resnet18(pretrained=True)\n",
    "print(resnet18_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83b536-cc20-40cf-b552-af9a076c17ff",
   "metadata": {},
   "source": [
    "### Klasyfikacja binarna - przygotowanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8f4ae-3303-4018-8842-a18f8c1bddea",
   "metadata": {},
   "source": [
    "Będziemy trenować model do klasyfikacji binarnej zdjęć pszczół i mrówek z wykorzystaniem transfer learningu.\n",
    "\n",
    "Zbiór ten jest dostępny do pobrania tutaj: https://download.pytorch.org/tutorial/hymenoptera_data.zip  \n",
    "(*hymenoptera* - *błonoskrzydłe* https://pl.wikipedia.org/wiki/B%C5%82onkoskrzyd%C5%82e)\n",
    "\n",
    "Zbiór ten należy rozpakować do katalogu `common/data` (w razie gdyby go jeszcze tam nie było)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53f09e1-b7bc-4e14-8023-da743017080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "DATA_PATH = pathlib.Path(\"common/data/hymenoptera_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a0b4a-c05e-4b07-bde0-3233203111fb",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Wczytaj zbiór zdjęć (dwa podbiory - train i val) wykorzystując klasę **ImageFolder** dataset dostępną w PyTorch (https://pytorch.org/vision/stable/datasets.html)\n",
    "\n",
    "Sprawdź rozmiar obu podzbiorów.  \n",
    "Sprawdź wymiary kilku wybranych zdjęć w zbiorze (używając możliwości klasy `Dataset`, nie przeglądając obrazki w katalogu).\n",
    "\n",
    "Sprawdź zawartość atrybutów klasy `ImageFolder` (https://pytorch.org/vision/stable/_modules/torchvision/datasets/folder.html#ImageFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2bde3ff-787b-432b-b917-6e3b04099e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_dataset = datasets.ImageFolder(DATA_PATH / 'train')\n",
    "val_dataset = datasets.ImageFolder(DATA_PATH / 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b343e503-c8f0-47c3-96af-72f269024bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba obrazów w zbiorze train: 244\n",
      "Liczba obrazów w zbiorze val: 153\n",
      "Obraz 0 - rozmiar: (768, 512)\n",
      "Obraz 1 - rozmiar: (500, 333)\n",
      "Obraz 2 - rozmiar: (500, 282)\n",
      "Obraz 3 - rozmiar: (500, 335)\n",
      "Obraz 4 - rozmiar: (500, 348)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Liczba obrazów w zbiorze train: {len(train_dataset)}\")\n",
    "print(f\"Liczba obrazów w zbiorze val: {len(val_dataset)}\")\n",
    "for i in range(5):\n",
    "    image, label = train_dataset[i]\n",
    "    print(f\"Obraz {i} - rozmiar: {image.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9ecb958-4df9-4ec3-b739-1158ec84ff9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klasy w zbiorze danych: ['ants', 'bees']\n",
      "Mapowanie klas na indeksy: {'ants': 0, 'bees': 1}\n",
      "Ścieżki do plików obrazów: [('common\\\\data\\\\hymenoptera_data\\\\train\\\\ants\\\\0013035.jpg', 0), ('common\\\\data\\\\hymenoptera_data\\\\train\\\\ants\\\\1030023514_aad5c608f9.jpg', 0), ('common\\\\data\\\\hymenoptera_data\\\\train\\\\ants\\\\1095476100_3906d8afde.jpg', 0), ('common\\\\data\\\\hymenoptera_data\\\\train\\\\ants\\\\1099452230_d1949d3250.jpg', 0), ('common\\\\data\\\\hymenoptera_data\\\\train\\\\ants\\\\116570827_e9c126745d.jpg', 0)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Klasy w zbiorze danych: {train_dataset.classes}\")\n",
    "print(f\"Mapowanie klas na indeksy: {train_dataset.class_to_idx}\")\n",
    "print(f\"Ścieżki do plików obrazów: {train_dataset.imgs[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3440cc1d-caba-4b52-ab95-1406ca9fd5f0",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Domyślnie `ImageFolder` dataset przechowuje obrazki jako `PIL.Image`. Należy je przekształcić do tensorów, aby ich użyć w treningu modeli.\n",
    "\n",
    "Odszukaj odpowiednią funkcję z `torchvision.transforms` (https://pytorch.org/vision/stable/transforms.html) i ponownie wczytaj zbiory danych z jej wykorzystaniem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e07b35de-2e0a-4d91-878e-e8bb85414199",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor()           \n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(DATA_PATH / 'train', transform=data_transforms)\n",
    "val_dataset = datasets.ImageFolder(DATA_PATH / 'val', transform=data_transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f8d25-eda5-404a-aa6f-0ef9107d0a64",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "W kolejnym kroku należy znormalizować wejściowe obrazki. Ponownie odszukaj odpowiednią transformację w `torchvision.transforms` i zbuduj listy transformacji `train_transforms` i `valid_transforms` z użyciem `transforms.Compose`. Wykorzystaj odpowiednie informacje z poniższej komórki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcf64ade-5906-4c2e-8381-9d19685e41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# średnie i odchylenia standardowe dla kanałów RGB dla zbioru uczącego ImageNet\n",
    "# ciekawostka: https://github.com/pytorch/vision/issues/1439\n",
    "IMAGENET_MEANS = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa2162a6-e7e9-44d7-a51c-381945613f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),          \n",
    "    transforms.Normalize(mean=IMAGENET_MEANS, std=IMAGENET_STD)  \n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(DATA_PATH / 'train', transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(DATA_PATH / 'val', transform=train_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a553af-5f09-4116-94a2-cfaf9ae95c9f",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Należy także doprowadzić oryginalne obrazki do odpowiednich wymiarów (224 na 224 piksele).\n",
    "\n",
    "W przypadku sieci (pre)trenowanych na danych ImageNet przyjęło się robić to dwukrokowo:\n",
    "- \"resize\" obrazka, aby krótszy wymiar miał długość 256\n",
    "- przycięcie (\"crop\") obrazka do jego środkowej części 224x224\n",
    "\n",
    "Rozszerz listę transformacji **podczas walidacji** zgodnie z powyższym opisem, wykorzystując odpowiednie funkcje z https://pytorch.org/vision/stable/transforms.html. Transformacjami treningowymi zajmiemy się w następnym ćwiczeniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f588da1-71ad-46be-8e02-a1162d432dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_IMG_SIZE = 224\n",
    "IMAGENET_RESIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88ad810b-dd2f-4f4c-baf1-fbc311a7c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMAGENET_RESIZE),               \n",
    "    transforms.CenterCrop(IMAGENET_IMG_SIZE),        \n",
    "    transforms.ToTensor(),                            \n",
    "    transforms.Normalize(mean=IMAGENET_MEANS, std=IMAGENET_STD) \n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(DATA_PATH / 'train', transform=train_transforms) \n",
    "val_dataset = datasets.ImageFolder(DATA_PATH / 'val', transform=train_transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c86cdc9-1b9b-4c80-b45a-5ff123591fcf",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Podczas treningu warto - zwłaszcza w przypadku posiadania niewielkiego zbioru danych - zastosować tzw. augmentację danych (więcej na kolejnych zajęciach).\n",
    "\n",
    "Zamiast \"sztywnego\" resize'owania obrazka i przycinania go względem środka, w czasie treningu:\n",
    "- dokonaj \"resize\" do losowej skali, a następnie przytnij do (losowego) fragmentu 224x224\n",
    "- dodatkowo losowo (domyślnie: prawdopodobieństwo 50%) przerzuć obrazek względem osi pionowej\n",
    "\n",
    "Znajdź odpowiednie funkcje w https://pytorch.org/vision/stable/transforms.html.\n",
    "Stwórz w ten sposób listę transformacji `train_transforms`.\n",
    "\n",
    "Wczytaj ponownie zbiór uczący i walidacyjny, podając odpowiednie listy transformacji do `ImageFolder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcdf0f6f-45c1-4ca9-bc9a-7ac1f982308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMAGENET_IMG_SIZE),  \n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.CenterCrop(IMAGENET_IMG_SIZE),                 \n",
    "    transforms.ToTensor(),                             \n",
    "    transforms.Normalize(mean=IMAGENET_MEANS, std=IMAGENET_STD)  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9c67625-2098-42e4-9cd7-e369297997a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(DATA_PATH / 'train', transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(DATA_PATH / 'val', transform=train_transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d410ce-76d7-450c-bc48-f0ccb273202c",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "\n",
    "Fine-tuningu modeli można dokonać na dwa główne sposoby:\n",
    "- dotrenować (optymalizować) wszystkie parametry (we wszystkich warstwach) pretrenowanego modelu\n",
    "- \"zamrozić\" pretrenowaną część modelu i dotrenować\n",
    "\n",
    "Na początek zajmiemy się pierwszym z wymienionych sposobów transfer learningu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a00d07-86d8-4586-ae22-1c0e29d39327",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Załaduj pretrenowaną sieć `resnet18` do zmiennej `model` i dostosuj ją do rozważanego problemu (co musisz zrobić?).\n",
    "Następnie uruchom trening modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4502d424-5eff-4f71-a0f9-6fadc478cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        epoch_loss = loss_train / len(train_loader)\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {epoch_loss}\")\n",
    "            \n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85b03e84-6ea0-4c03-bdd6-e5b7de4cb0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_classes = 2\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "model = model.to(device)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# training_loop(n_epochs=10, optimizer=optimizer, model=model, loss_fn=loss_fn, train_loader=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a940c0c-930d-46e8-b412-247177665ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 0.696533739566803\n",
      "Epoch 5, Training loss 0.11297923792153597\n",
      "Epoch 10, Training loss 0.06457358552142978\n",
      "Epoch 15, Training loss 0.05301452940329909\n",
      "Epoch 20, Training loss 0.04594590375199914\n",
      "Epoch 25, Training loss 0.08516848087310791\n",
      "Training complete in 1m 13s\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# SGD with momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 25,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf088a4a-741a-42d5-a9ba-c39f02bb7bde",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Sprawdź jakość wytrenowanego modelu uruchamiając poniższe komórki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4dfe483d-5c1c-400a-b556-feefa4558756",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f327f88-2f78-48a6-9ad6-a5b0e835d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, train_loader, val_loader):\n",
    "    model.eval()\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((preds == labels).sum())\n",
    "                \n",
    "        print(f\"{name} accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "878bfe02-f79e-4450-993c-0256d74d0368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9549180327868853\n",
      "val accuracy: 0.9150326797385621\n"
     ]
    }
   ],
   "source": [
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13af6f0f-aa61-4742-9d3b-ecd4c400084c",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Jeszcze raz załaduj i przygotuj model `resnet18` (np. do zmiennej `model_frozen`, tym razem \"zamrażając\" wszystkie pretrenowane warstwy modelu.\n",
    "Wytrenuj model i sprawdź jego dokładność."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2eac2545-1d4e-4ba4-8056-36016db0d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_frozen = models.resnet18(pretrained=True)\n",
    "\n",
    "for param in model_frozen.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_classes = 2\n",
    "num_ftrs = model_frozen.fc.in_features\n",
    "model_frozen.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "model_frozen = model_frozen.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "090e74b8-5298-481c-ada4-df0e460ee0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 0.6365495771169662\n",
      "Epoch 5, Training loss 0.16748459078371525\n",
      "Epoch 10, Training loss 0.12793205678462982\n",
      "Epoch 15, Training loss 0.09340222552418709\n",
      "Epoch 20, Training loss 0.07861949596554041\n",
      "Epoch 25, Training loss 0.13101957738399506\n",
      "Training complete in 0m 48s\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# SGD with momentum\n",
    "optimizer = torch.optim.SGD(model_frozen.parameters(), lr=1e-2, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 25,\n",
    "    optimizer = optimizer,\n",
    "    model = model_frozen,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e46b7d01-8126-4b1e-86d7-5c177812e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2d15f3b-a05b-4df7-9a10-600107529add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9713114754098361\n",
      "val accuracy: 0.9215686274509803\n"
     ]
    }
   ],
   "source": [
    "validate(model_frozen, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a80497-5b7b-4408-8386-ec185f3c1c37",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Porównaj powyższe wyniki z uzyskanymi dla modelu `Net` stworzonego na wcześniejszych zajęciach (lekko zmodyfikowane wymiary dla warstw gęstych - inny rozmiar obrazków wejściowych)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9dcc4290-a886-4b7e-8746-e3babacff921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 56 * 56, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 56 * 56)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18f9fe6d-2fee-480d-8acd-c9c5401c7d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 0.6748300045728683\n",
      "Epoch 5, Training loss 0.6375532001256943\n",
      "Epoch 10, Training loss 0.6169845461845398\n",
      "Epoch 15, Training loss 0.6241620928049088\n",
      "Epoch 20, Training loss 0.6095932871103287\n",
      "Epoch 25, Training loss 0.5838529914617538\n",
      "Training complete in 0m 35s\n"
     ]
    }
   ],
   "source": [
    "net_model = Net()\n",
    "net_model = net_model.to(device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# SGD with momentum\n",
    "optimizer = torch.optim.SGD(net_model.parameters(), lr=1e-2, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 25,\n",
    "    optimizer = optimizer,\n",
    "    model = net_model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "753720a6-5e69-4ccc-afe0-4eb63bfce9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.6721311475409836\n",
      "val accuracy: 0.6339869281045751\n"
     ]
    }
   ],
   "source": [
    "validate(net_model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
