{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sztuczne sieci neuronowe - laboratorium 8\n",
    "\n",
    "### Splotowe sieci neuronowe - cz. 2\n",
    "\n",
    "Na poprzednich zajęciach poznaliśmy warstwy tworzące **splotową sieć neuronową** i nauczyliśmy się tworzyć modele jako klasy  dziedziczące po `nn.Module`.\n",
    "\n",
    "Dziś wytrenujemy splotową sieć neuronową do binarnej klasyfikacji obrazu i poznamy dodatkowe techniki stosowane w sieciach neuronowych, m.in. do regularyzacji modeli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytania kontrolne\n",
    "\n",
    "1. Opisz budowę splotowej sieci neuronowej. Wyjaśnij, do czego służą jej poszczególne warstwy.\n",
    "2. Na czym polega regularyzacja modeli?\n",
    "3. Jakie znasz metody regularyzacji stosowane w sieciach neuronowych?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z poprzednich ćwiczeń\n",
    "\n",
    "Uruchom kolejne komórki, wykorzystujące kod z poprzednich zajęć, aby przygotować zbiór danych - `cifar2` oraz klasę `Net` definiującą model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    0: \"airplane\", \n",
    "    1: \"automobile\", \n",
    "    2: \"bird\", \n",
    "    3: \"cat\", \n",
    "    4: \"deer\", \n",
    "    5: \"dog\", \n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cifar10 = datasets.CIFAR10(\"data\", train=True, download=False, transform=transforms.ToTensor())\n",
    "tensor_cifar10_val = datasets.CIFAR10(\"data\", train=False, download=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
    "per_channel_means = imgs.view(3, -1).mean(dim=1)\n",
    "per_channel_std = imgs.view(3, -1).std(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_compose = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(per_channel_means, per_channel_std)\n",
    "])\n",
    "\n",
    "transformed_cifar10 = datasets.CIFAR10(\"data\", train=True, download=False, transform=transforms_compose)\n",
    "transformed_cifar10_val = datasets.CIFAR10(\"data\", train=False, download=False, transform=transforms_compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "new_class_names  = [class_names[i] for i in label_map]\n",
    "\n",
    "cifar2 = [(img, label_map[label]) for img, label in tensor_cifar10 if label in label_map]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in tensor_cifar10 if label in label_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "Uzupełnij poniższe komórki, aby wytrenować splotową sieć neuronową do zadania klasyfikacji binarnej.\n",
    "\n",
    "Przyjmij learning rate o wartości 0.01 i batch size 64. Trenuj przez 100 epok. Użyj optymaliatora SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcifar2_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 105.95751374959946\n",
      "Epoch 11, Training loss 65.94430935382843\n",
      "Epoch 21, Training loss 55.472624346613884\n",
      "Epoch 31, Training loss 51.47982883453369\n",
      "Epoch 41, Training loss 48.44425129890442\n",
      "Epoch 51, Training loss 45.559572488069534\n",
      "Epoch 61, Training loss 43.25438152253628\n",
      "Epoch 71, Training loss 41.051490783691406\n",
      "Epoch 81, Training loss 39.26327306777239\n",
      "Epoch 91, Training loss 36.70376615971327\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "train_loader = DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walidacja modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Zaimplementuj funkcję `validate`, która zmierzy dokładność wytrenowanego modelu na dwóch zbiorach - uczącym i walidacyjnym.\n",
    "\n",
    "Porównaj wyniki z wynikami z laboratorium nr 5 (sieć gęsta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, train_loader, val_loader):\n",
    "    model.eval()\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "        print(f\"{name} accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9033\n",
      "val accuracy: 0.9033\n"
     ]
    }
   ],
   "source": [
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zapis i odczyt modelu\n",
    "\n",
    "Wytrenowany model (zwłaszcza tak, którego trening trwa długo) warto zapisać, aby móc go użyć później.\n",
    "\n",
    "Typowo po każdej epoce treningu sprawdza się działanie modelu na zbiorze walidacyjnym (walidacja).\n",
    "Zapisu modelu (tzw. \"checkpoint\") typowo dokonuje się, jeśli wartość danej metryki (np. dokładność lub F1 na zbiorze walidacyjnym) jest lepsza niż najlepsza uzyskana dotychczas.\n",
    "\n",
    "PyTorch pozwala zapisać wagi (parametry) modelu z użyciem `torch.save` oraz tzw. `state_dict` modelu (https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html). Innym sposobem zapisu jest zapis całego modelu (z użyciem `pickle` \"pod spodem\"):\n",
    "https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html\n",
    "\n",
    "Następnie, do wczytania zapisanego modelu można użyć metody `load_state_dict` (oraz `torch.load`), jeśli zapisywaliśmy tylko `state_dict` lub tylko `torch.load`, jeśli zapisywaliśmy cały model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[ 0.1399, -0.2607, -0.0720],\n",
      "          [-0.2648, -0.3619, -0.2403],\n",
      "          [-0.2758, -0.3072, -0.3164]],\n",
      "\n",
      "         [[ 0.0725,  0.1068,  0.2232],\n",
      "          [ 0.0172, -0.0714,  0.0909],\n",
      "          [ 0.0788, -0.1914, -0.2757]],\n",
      "\n",
      "         [[ 0.3221,  0.3741,  0.0511],\n",
      "          [ 0.0111,  0.2005,  0.2469],\n",
      "          [ 0.0897, -0.0769,  0.2289]]],\n",
      "\n",
      "\n",
      "        [[[-0.0833,  0.0216,  0.0188],\n",
      "          [ 0.2041, -0.0078, -0.3790],\n",
      "          [ 0.0608, -0.0866, -0.2872]],\n",
      "\n",
      "         [[-0.0092,  0.1148, -0.0330],\n",
      "          [ 0.2250,  0.1591, -0.2070],\n",
      "          [ 0.2663,  0.1208,  0.0754]],\n",
      "\n",
      "         [[ 0.2024, -0.1487, -0.3180],\n",
      "          [ 0.3235, -0.0661, -0.3352],\n",
      "          [ 0.2288, -0.1104, -0.2788]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2502,  0.2309,  0.2779],\n",
      "          [ 0.3580,  0.2259, -0.0228],\n",
      "          [-0.0776,  0.0790, -0.0452]],\n",
      "\n",
      "         [[-0.2362, -0.1300, -0.2186],\n",
      "          [-0.1832,  0.0413, -0.2576],\n",
      "          [-0.1222, -0.2921, -0.2645]],\n",
      "\n",
      "         [[ 0.1345, -0.0688,  0.2143],\n",
      "          [ 0.2496, -0.0082,  0.2461],\n",
      "          [-0.1511, -0.0213, -0.1069]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3029, -0.0398,  0.2208],\n",
      "          [ 0.2337,  0.1378,  0.0016],\n",
      "          [-0.1043,  0.0537, -0.2314]],\n",
      "\n",
      "         [[-0.1952, -0.0563, -0.1049],\n",
      "          [-0.0721, -0.0949,  0.0606],\n",
      "          [-0.3844, -0.2591, -0.3399]],\n",
      "\n",
      "         [[ 0.1323, -0.0381,  0.2843],\n",
      "          [ 0.1366,  0.1457,  0.0148],\n",
      "          [ 0.0043,  0.0050,  0.2235]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2752, -0.1923, -0.0701],\n",
      "          [-0.2186, -0.2510,  0.1195],\n",
      "          [-0.0936, -0.1711, -0.0250]],\n",
      "\n",
      "         [[ 0.2331, -0.0585,  0.0843],\n",
      "          [-0.1571, -0.0458, -0.2106],\n",
      "          [-0.0792, -0.2186,  0.0560]],\n",
      "\n",
      "         [[ 0.2734,  0.2356,  0.1050],\n",
      "          [ 0.0659, -0.1116,  0.1736],\n",
      "          [-0.0337, -0.0781, -0.1860]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0772, -0.0824,  0.0862],\n",
      "          [ 0.0796,  0.2619,  0.1838],\n",
      "          [ 0.2326,  0.1743,  0.0038]],\n",
      "\n",
      "         [[-0.3645, -0.3692, -0.2386],\n",
      "          [ 0.0726,  0.2109, -0.0008],\n",
      "          [-0.0263, -0.1964,  0.1027]],\n",
      "\n",
      "         [[-0.0561, -0.2458, -0.1998],\n",
      "          [ 0.2718,  0.3151,  0.2839],\n",
      "          [ 0.0291, -0.1462, -0.1368]]],\n",
      "\n",
      "\n",
      "        [[[-0.2813, -0.1328,  0.1037],\n",
      "          [-0.2456, -0.1592, -0.2237],\n",
      "          [ 0.1320,  0.2124,  0.0506]],\n",
      "\n",
      "         [[-0.2454,  0.0255, -0.2718],\n",
      "          [-0.0416,  0.0599,  0.0806],\n",
      "          [ 0.2646,  0.1507,  0.2572]],\n",
      "\n",
      "         [[-0.0833,  0.0291, -0.1791],\n",
      "          [ 0.1791, -0.0259, -0.0354],\n",
      "          [ 0.1499,  0.0049,  0.2277]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2337, -0.2310,  0.0029],\n",
      "          [ 0.1063, -0.0234, -0.0719],\n",
      "          [-0.1929, -0.0453,  0.1063]],\n",
      "\n",
      "         [[ 0.1240, -0.1963, -0.1212],\n",
      "          [ 0.1446,  0.2137,  0.0333],\n",
      "          [ 0.0100,  0.1064,  0.0926]],\n",
      "\n",
      "         [[ 0.1064, -0.1943,  0.1090],\n",
      "          [-0.0799,  0.0716, -0.2009],\n",
      "          [-0.0363, -0.1395,  0.0362]]],\n",
      "\n",
      "\n",
      "        [[[-0.0488,  0.1403,  0.1758],\n",
      "          [ 0.2006,  0.0024,  0.2052],\n",
      "          [ 0.1918,  0.2820,  0.1142]],\n",
      "\n",
      "         [[ 0.1914, -0.0851, -0.1015],\n",
      "          [ 0.2217, -0.0873,  0.1084],\n",
      "          [-0.1080,  0.0188,  0.0173]],\n",
      "\n",
      "         [[-0.2869, -0.1037, -0.2816],\n",
      "          [-0.0899, -0.4059, -0.4408],\n",
      "          [-0.2970, -0.1927, -0.2893]]],\n",
      "\n",
      "\n",
      "        [[[-0.1765,  0.0009,  0.2289],\n",
      "          [ 0.0307, -0.0611,  0.1652],\n",
      "          [ 0.1285,  0.1466, -0.1435]],\n",
      "\n",
      "         [[-0.0578,  0.1397, -0.0486],\n",
      "          [-0.1443, -0.0513,  0.1243],\n",
      "          [ 0.3143, -0.1687, -0.1296]],\n",
      "\n",
      "         [[-0.0917, -0.1546,  0.1584],\n",
      "          [-0.0153, -0.3900, -0.0517],\n",
      "          [-0.0321, -0.2390,  0.0224]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1206,  0.2056,  0.0273],\n",
      "          [-0.1407, -0.0134, -0.1904],\n",
      "          [-0.0731,  0.0483,  0.1642]],\n",
      "\n",
      "         [[ 0.1324,  0.2029, -0.1348],\n",
      "          [-0.2137,  0.2246, -0.1563],\n",
      "          [-0.0490,  0.2698,  0.0442]],\n",
      "\n",
      "         [[ 0.0520,  0.0823, -0.2170],\n",
      "          [-0.2355, -0.1888,  0.0171],\n",
      "          [ 0.0077, -0.0785, -0.1766]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1882,  0.1990,  0.1975],\n",
      "          [-0.1939, -0.1222, -0.1340],\n",
      "          [-0.0539,  0.0957, -0.0691]],\n",
      "\n",
      "         [[ 0.0080,  0.1482,  0.0185],\n",
      "          [-0.2221, -0.0220, -0.2095],\n",
      "          [ 0.1808,  0.2204, -0.1135]],\n",
      "\n",
      "         [[-0.0428, -0.0043, -0.1496],\n",
      "          [-0.1483,  0.1798, -0.1696],\n",
      "          [ 0.0925,  0.3070,  0.0431]]],\n",
      "\n",
      "\n",
      "        [[[-0.1062,  0.0502, -0.1031],\n",
      "          [-0.0836,  0.1240,  0.0449],\n",
      "          [-0.1712, -0.0082, -0.0237]],\n",
      "\n",
      "         [[ 0.2374,  0.1090,  0.2452],\n",
      "          [-0.2782,  0.0171, -0.0222],\n",
      "          [-0.2548, -0.0660, -0.1457]],\n",
      "\n",
      "         [[-0.0036,  0.1788,  0.2366],\n",
      "          [-0.2682,  0.0985, -0.1188],\n",
      "          [-0.1138,  0.1038, -0.2425]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1358,  0.2465,  0.1675],\n",
      "          [ 0.0123,  0.0522,  0.0834],\n",
      "          [ 0.1396,  0.2271,  0.2456]],\n",
      "\n",
      "         [[-0.0671, -0.1340, -0.0227],\n",
      "          [-0.0888,  0.1208,  0.1203],\n",
      "          [-0.2606, -0.1978, -0.0277]],\n",
      "\n",
      "         [[-0.2766, -0.1267,  0.0095],\n",
      "          [-0.3416, -0.1866,  0.1127],\n",
      "          [-0.2375, -0.0356,  0.1041]]],\n",
      "\n",
      "\n",
      "        [[[-0.1677, -0.0955,  0.1784],\n",
      "          [ 0.0880,  0.1323,  0.0876],\n",
      "          [-0.1001, -0.0554, -0.1011]],\n",
      "\n",
      "         [[ 0.0728,  0.0327,  0.1244],\n",
      "          [-0.1567,  0.0109, -0.1515],\n",
      "          [ 0.1294,  0.1843,  0.0037]],\n",
      "\n",
      "         [[ 0.1406,  0.1409,  0.0514],\n",
      "          [ 0.1519, -0.1505, -0.1483],\n",
      "          [-0.1227,  0.1320,  0.0302]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2335,  0.2132, -0.1230],\n",
      "          [ 0.0094, -0.0694,  0.3171],\n",
      "          [-0.1504, -0.1729,  0.2013]],\n",
      "\n",
      "         [[-0.1972,  0.1146, -0.1294],\n",
      "          [-0.2532, -0.1876,  0.2202],\n",
      "          [-0.2357, -0.1978,  0.1012]],\n",
      "\n",
      "         [[-0.0912, -0.0042, -0.1266],\n",
      "          [-0.2771, -0.0121,  0.2248],\n",
      "          [-0.0783, -0.2263, -0.0489]]]])), ('conv1.bias', tensor([ 0.1182,  0.0049, -0.1369, -0.0545,  0.1550, -0.1785,  0.0460, -0.0072,\n",
      "         0.2851, -0.0319,  0.0703, -0.1693,  0.2863, -0.0744, -0.2487,  0.3193])), ('conv2.weight', tensor([[[[ 0.0128, -0.0379,  0.0278],\n",
      "          [-0.0699,  0.0721, -0.1343],\n",
      "          [ 0.1025,  0.0636, -0.0822]],\n",
      "\n",
      "         [[-0.0678,  0.0881, -0.0149],\n",
      "          [ 0.2182,  0.2828,  0.0941],\n",
      "          [ 0.2072,  0.0581,  0.0141]],\n",
      "\n",
      "         [[-0.0630, -0.0736, -0.0438],\n",
      "          [ 0.0260, -0.0222, -0.2015],\n",
      "          [ 0.1149, -0.1039, -0.1309]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1124, -0.0847,  0.0893],\n",
      "          [-0.0084, -0.0935,  0.0207],\n",
      "          [-0.0422,  0.0864,  0.1573]],\n",
      "\n",
      "         [[ 0.1110, -0.0556, -0.0625],\n",
      "          [ 0.0587,  0.0187, -0.0959],\n",
      "          [ 0.0119, -0.0551,  0.0162]],\n",
      "\n",
      "         [[-0.0486, -0.0345,  0.0182],\n",
      "          [ 0.0062,  0.0659,  0.1116],\n",
      "          [ 0.1229,  0.0417,  0.0366]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1729,  0.0742,  0.0728],\n",
      "          [-0.0132,  0.1850,  0.0747],\n",
      "          [-0.0304, -0.0512, -0.0796]],\n",
      "\n",
      "         [[-0.0096,  0.0294, -0.0242],\n",
      "          [-0.0019, -0.0610,  0.0278],\n",
      "          [-0.0670, -0.2057, -0.1484]],\n",
      "\n",
      "         [[ 0.0795,  0.1270,  0.1100],\n",
      "          [ 0.2127,  0.2030,  0.2389],\n",
      "          [ 0.0360,  0.0488,  0.0343]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0798,  0.0070, -0.0239],\n",
      "          [ 0.0071,  0.1403,  0.0428],\n",
      "          [-0.0646,  0.0383,  0.0889]],\n",
      "\n",
      "         [[ 0.0240,  0.0867,  0.0111],\n",
      "          [ 0.0668, -0.0197,  0.0366],\n",
      "          [ 0.0091,  0.0131, -0.1249]],\n",
      "\n",
      "         [[-0.0175,  0.0136,  0.1344],\n",
      "          [-0.0767, -0.0523,  0.1257],\n",
      "          [ 0.0518,  0.0428, -0.0836]]],\n",
      "\n",
      "\n",
      "        [[[-0.1417,  0.0147,  0.0891],\n",
      "          [-0.0151,  0.1639,  0.1660],\n",
      "          [ 0.1018,  0.0507,  0.1843]],\n",
      "\n",
      "         [[-0.0207, -0.0374, -0.0912],\n",
      "          [-0.1017, -0.0176, -0.0609],\n",
      "          [-0.0494, -0.1259,  0.0241]],\n",
      "\n",
      "         [[ 0.0348,  0.0093, -0.0359],\n",
      "          [ 0.0349, -0.0277,  0.0224],\n",
      "          [ 0.1148,  0.0208,  0.0848]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0365,  0.0208, -0.0174],\n",
      "          [-0.0951, -0.1111, -0.1102],\n",
      "          [-0.0623,  0.0487, -0.0475]],\n",
      "\n",
      "         [[ 0.0432, -0.0100, -0.0728],\n",
      "          [ 0.0563,  0.0544, -0.0412],\n",
      "          [-0.0502, -0.0638, -0.0503]],\n",
      "\n",
      "         [[-0.0468,  0.0028, -0.0095],\n",
      "          [-0.1400,  0.0273, -0.0364],\n",
      "          [-0.0363, -0.0123,  0.0944]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.2034, -0.2461, -0.0315],\n",
      "          [-0.2001, -0.0710, -0.0102],\n",
      "          [-0.2277, -0.2172, -0.0786]],\n",
      "\n",
      "         [[ 0.0636,  0.0327, -0.0336],\n",
      "          [ 0.0573,  0.0295, -0.0981],\n",
      "          [ 0.0740,  0.1388, -0.0931]],\n",
      "\n",
      "         [[ 0.0410,  0.1789,  0.0559],\n",
      "          [ 0.1199,  0.0611,  0.1056],\n",
      "          [ 0.1060,  0.0882,  0.1213]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1034,  0.2752,  0.1655],\n",
      "          [ 0.1589,  0.2813,  0.1971],\n",
      "          [-0.0072,  0.0907,  0.2610]],\n",
      "\n",
      "         [[ 0.0448, -0.0737,  0.0392],\n",
      "          [-0.0394, -0.0974,  0.0119],\n",
      "          [ 0.0707, -0.0862,  0.0146]],\n",
      "\n",
      "         [[-0.0123,  0.2127,  0.1207],\n",
      "          [ 0.0484,  0.1948,  0.1061],\n",
      "          [-0.0522,  0.0649,  0.0726]]],\n",
      "\n",
      "\n",
      "        [[[-0.1676,  0.0742,  0.0279],\n",
      "          [-0.0203,  0.0633, -0.0137],\n",
      "          [ 0.1242,  0.0100, -0.0566]],\n",
      "\n",
      "         [[ 0.0887,  0.0395,  0.0033],\n",
      "          [ 0.0726, -0.0150,  0.0809],\n",
      "          [ 0.0060,  0.0791,  0.0832]],\n",
      "\n",
      "         [[-0.1884, -0.1032, -0.1545],\n",
      "          [ 0.0684,  0.0367,  0.0116],\n",
      "          [ 0.0670,  0.1423,  0.0065]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0808, -0.0293, -0.0133],\n",
      "          [ 0.0073, -0.0315, -0.0339],\n",
      "          [ 0.0194, -0.0427, -0.0789]],\n",
      "\n",
      "         [[ 0.0497, -0.1047, -0.0431],\n",
      "          [ 0.0033,  0.1026,  0.0396],\n",
      "          [ 0.0301, -0.0154,  0.0849]],\n",
      "\n",
      "         [[ 0.1521,  0.1530, -0.0135],\n",
      "          [-0.0518, -0.0360, -0.0962],\n",
      "          [-0.0080, -0.0306, -0.0810]]],\n",
      "\n",
      "\n",
      "        [[[-0.0623,  0.0932, -0.0197],\n",
      "          [ 0.0125, -0.0295, -0.0624],\n",
      "          [ 0.0682, -0.0440,  0.1167]],\n",
      "\n",
      "         [[ 0.1006, -0.0878,  0.0390],\n",
      "          [ 0.0139,  0.0231,  0.0889],\n",
      "          [-0.0286,  0.1332, -0.0127]],\n",
      "\n",
      "         [[-0.0878, -0.0204, -0.0144],\n",
      "          [-0.0626, -0.0776, -0.0535],\n",
      "          [ 0.1341,  0.1192,  0.0359]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0611,  0.0331,  0.0010],\n",
      "          [-0.0335,  0.0643,  0.0035],\n",
      "          [ 0.1018,  0.1052,  0.0593]],\n",
      "\n",
      "         [[-0.0314,  0.0319,  0.0867],\n",
      "          [-0.0214,  0.0530,  0.0427],\n",
      "          [ 0.1230,  0.0269,  0.0361]],\n",
      "\n",
      "         [[-0.0206, -0.0632, -0.1127],\n",
      "          [ 0.0375, -0.0765, -0.1150],\n",
      "          [ 0.0078,  0.0477,  0.0061]]]])), ('conv2.bias', tensor([ 0.0111, -0.0744, -0.0950, -0.0341,  0.0363,  0.1374, -0.0264, -0.2244])), ('fc1.weight', tensor([[ 3.0960e-02, -1.9301e-02, -8.4170e-03,  ...,  3.0596e-02,\n",
      "          2.6303e-02,  1.9407e-02],\n",
      "        [ 1.5756e-02, -8.1372e-02, -8.6432e-03,  ..., -2.9998e-02,\n",
      "          6.0941e-03, -5.7119e-05],\n",
      "        [ 3.2306e-02, -8.3487e-03, -5.6566e-02,  ..., -1.4331e-03,\n",
      "         -3.2568e-02,  2.4605e-02],\n",
      "        ...,\n",
      "        [ 4.5871e-02, -2.7412e-02,  1.8909e-02,  ...,  2.1202e-02,\n",
      "         -1.1061e-02,  2.5452e-02],\n",
      "        [ 1.2068e-02, -1.6910e-02,  6.0799e-03,  ..., -3.8442e-02,\n",
      "          2.6612e-02, -2.8886e-02],\n",
      "        [ 2.0191e-02,  3.7790e-02, -1.6094e-02,  ..., -1.0421e-02,\n",
      "         -5.6140e-04, -1.3207e-02]])), ('fc1.bias', tensor([-0.0672,  0.0046,  0.0211, -0.0673, -0.0677, -0.0918, -0.0656,  0.0077,\n",
      "         0.0414,  0.0202, -0.0620,  0.1350, -0.0096,  0.0406, -0.0106, -0.0112,\n",
      "        -0.0870, -0.0185, -0.0717,  0.0398, -0.0633,  0.0056, -0.0060,  0.0261,\n",
      "        -0.0456, -0.0339, -0.0081, -0.1111, -0.0132,  0.0424, -0.0295, -0.0253])), ('fc2.weight', tensor([[-0.1761,  0.3248,  0.3826, -0.0986, -0.2640, -0.1709,  0.0039,  0.1048,\n",
      "          0.1578,  0.1329, -0.4600,  0.7184, -0.6338,  0.5022,  0.5262,  0.4234,\n",
      "          0.5601, -0.0928, -0.3988,  0.1790, -0.5923,  0.3571,  0.0349,  0.2923,\n",
      "         -0.3006,  0.0572, -0.1492, -0.2772, -0.3209,  0.1759,  0.0151,  0.0676],\n",
      "        [ 0.1389, -0.4443, -0.2208,  0.2233,  0.0779,  0.2763,  0.2284,  0.2415,\n",
      "          0.0710, -0.3257,  0.3245, -0.7227,  0.4694, -0.4180, -0.5566, -0.3007,\n",
      "         -0.5376, -0.1556,  0.2630, -0.0899,  0.5426, -0.1125, -0.0680, -0.0915,\n",
      "          0.1705,  0.2252,  0.1437,  0.4690,  0.3160, -0.2137, -0.0087,  0.1011]])), ('fc2.bias', tensor([ 0.3228, -0.1096]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"data/birds_vs_airplanes.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net()\n",
    "loaded_model.load_state_dict(torch.load(\"data/birds_vs_airplanes.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening na GPU (opcjonalnie)\n",
    "\n",
    "Aby przyspieszyć trening (zwłaszcza w przypadku głębokich modeli i dużych zbiorów danych), powszechnie stosuje się karty graficzne (GPU). Jeśli mamy dostęp do maszyny z kartą graficzną (najlepiej od NVIDIA, obsługującą CUDA), możemy łatwo \"przenieść\" trening na GPU.\n",
    "\n",
    "W tym celu należy przenieść zarówno dane, jak i model, na kartę graficzną, używając metody `.to` (tensora i `nn.Module`) na zdefiniowane urządzenie (patrz poniżej)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Zmodyfikuj napisaną wyżej pętlę treningową oraz inicjalizację modelu, przenosząc odpowiednio dane (obrazki i etykiety) oraz model na `device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 0.683061994564761\n",
      "Epoch 10, Training loss 0.472398551026727\n",
      "Epoch 20, Training loss 0.3635449573682372\n",
      "Epoch 30, Training loss 0.33564495651205634\n",
      "Epoch 40, Training loss 0.30892097000863145\n",
      "Epoch 50, Training loss 0.28991292986520534\n",
      "Epoch 60, Training loss 0.2725954226627471\n",
      "Epoch 70, Training loss 0.2589002258268891\n",
      "Epoch 80, Training loss 0.24738430929411748\n",
      "Epoch 90, Training loss 0.23359656447817564\n",
      "Epoch 100, Training loss 0.22354738153279965\n"
     ]
    }
   ],
   "source": [
    "model_gpu = Net().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_gpu.parameters(), lr=0.01)\n",
    "train_loader = DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "training_loop(n_epochs=100, optimizer=optimizer, model=model_gpu, loss_fn=loss_fn, train_loader=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rozbudowa modelu\n",
    "\n",
    "Możemy \"powiększyć\" model \"na szerokość\" (dodać więcej filtrów) lub \"na głębokość\" (dodać więcej warstw)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Zmodyfikuj klasę `Net` i stwórz kolejno:\n",
    "- `NetWidth` - 2x więcej filtrów w warstwach splotowych (niech liczba filtrów będzie argumentem konstruktora)\n",
    "- `NetDepth` - dodatkowa warstwa splotowa `conv3`\n",
    "\n",
    "Pamiętaj o zmodyfikowaniu metody `forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1*2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1*2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans*2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1, n_chans1, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularyzacja L2 / weight decay\n",
    "\n",
    "Regularyzację L2 można zaimplementować samemu (jak niżej). \n",
    "\n",
    "Jest ona jednak wbudowana w `torch.optim`, np. https://pytorch.org/docs/stable/optim.html#torch.optim.SGD, gdzie wystarczy podać wartość `weight_decay` tworząc optymalizator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "W poniższej pętli treningowej dopisz fragment realizujący regularyzację L2 dla lambda = 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "                        \n",
    "            l2_lambda = 0.001\n",
    "            for param in model.parameters():\n",
    "                loss += l2_lambda * torch.norm(param)**2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 110.4469341635704\n",
      "Epoch 10, Training loss 75.77257165312767\n",
      "Epoch 20, Training loss 62.245709985494614\n",
      "Epoch 30, Training loss 55.95564502477646\n",
      "Epoch 40, Training loss 53.32600408792496\n",
      "Epoch 50, Training loss 50.80273789167404\n",
      "Epoch 60, Training loss 48.787751004099846\n",
      "Epoch 70, Training loss 47.15585224330425\n",
      "Epoch 80, Training loss 45.417018070816994\n",
      "Epoch 90, Training loss 44.2782152146101\n",
      "Epoch 100, Training loss 42.82187984883785\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Wywołaj \"zwykłą\" pętlę treningową, tym razem podająć `weight_decay` optyamlizatora równe 0.001. Zaobserwuj wpływ na funkcję straty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "Dołóż warstwy `nn.Dropout2d` po warstwach splotowych (po max poolingu) do sieci `Net` i stwórz w ten sposob `NetDropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.dropout(out)\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = self.dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Dodaj warstwy `BatchNorm2d` po warstwach splotowych `Net` tworząc w ten sposób `NetBatchNorm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()        \n",
    "        self.n_chans = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(n_chans1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.bn1(self.conv1(x))), 2)\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.bn2(self.conv2(out))), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie (zadanie domowe)\n",
    "\n",
    "Porównaj wyniki uzyskane przez każdą z zaimplementowanych klas na zbiorze uczącym i walidacyjnym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 111.15281909704208\n",
      "Epoch 10, Training loss 72.56290775537491\n",
      "Epoch 20, Training loss 60.283688858151436\n",
      "Epoch 30, Training loss 56.228682935237885\n",
      "Epoch 40, Training loss 53.87071466445923\n",
      "Epoch 50, Training loss 52.15117555856705\n",
      "Epoch 60, Training loss 50.45057292282581\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m Net()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtraining_loop_l2reg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWalidacja modelu: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mtype()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m validate(model, train_loader, val_loader)\n",
      "Cell \u001b[1;32mIn[60], line 15\u001b[0m, in \u001b[0;36mtraining_loop_l2reg\u001b[1;34m(n_epochs, optimizer, model, loss_fn, train_loader)\u001b[0m\n\u001b[0;32m     12\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l2_lambda \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(param)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")\n",
    "print(f\"Walidacja modelu: {model.type()}\")\n",
    "validate(model, train_loader, val_loader)\n",
    "\n",
    "\n",
    "model_NetWidth= NetWidth().to(device)\n",
    "optimizer = torch.optim.SGD(model_NetWidth.parameters(), lr=1e-2)\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model_NetWidth,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")\n",
    "print(f\"Walidacja modelu: {model_NetWidth.type()}\")\n",
    "validate(model_NetWidth, train_loader, val_loader)\n",
    "\n",
    "\n",
    "model_NetDepth = NetDepth().to(device)\n",
    "optimizer = torch.optim.SGD(model_NetDepth.parameters(), lr=1e-2)\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model_NetDepth,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")\n",
    "print(f\"Walidacja modelu: {model_NetDepth.type()}\")\n",
    "validate(model_NetDepth, train_loader, val_loader)\n",
    "\n",
    "\n",
    "model_NetDropout = NetDropout().to(device)\n",
    "optimizer = torch.optim.SGD(model_NetDropout.parameters(), lr=1e-2)\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model_NetDropout,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")\n",
    "print(f\"Walidacja modelu: {model_NetDropout.type()}\")\n",
    "validate(model_NetDropout, train_loader, val_loader)\n",
    "\n",
    "model_NetBatchNorm = NetBatchNorm().to(device)\n",
    "optimizer = torch.optim.SGD(model_NetBatchNorm.parameters(), lr=1e-2)\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model_NetBatchNorm,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")\n",
    "print(f\"Walidacja modelu: {model_NetBatchNorm.type()}\")\n",
    "validate(model_NetBatchNorm, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
