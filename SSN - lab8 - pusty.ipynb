{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sztuczne sieci neuronowe - laboratorium 8\n",
    "\n",
    "### Splotowe sieci neuronowe - cz. 2\n",
    "\n",
    "Na poprzednich zajęciach poznaliśmy warstwy tworzące **splotową sieć neuronową** i nauczyliśmy się tworzyć modele jako klasy  dziedziczące po `nn.Module`.\n",
    "\n",
    "Dziś wytrenujemy splotową sieć neuronową do binarnej klasyfikacji obrazu i poznamy dodatkowe techniki stosowane w sieciach neuronowych, m.in. do regularyzacji modeli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytania kontrolne\n",
    "\n",
    "1. Opisz budowę splotowej sieci neuronowej. Wyjaśnij, do czego służą jej poszczególne warstwy.\n",
    "2. Na czym polega regularyzacja modeli?\n",
    "3. Jakie znasz metody regularyzacji stosowane w sieciach neuronowych?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z poprzednich ćwiczeń\n",
    "\n",
    "Uruchom kolejne komórki, wykorzystujące kod z poprzednich zajęć, aby przygotować zbiór danych - `cifar2` oraz klasę `Net` definiującą model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    0: \"airplane\", \n",
    "    1: \"automobile\", \n",
    "    2: \"bird\", \n",
    "    3: \"cat\", \n",
    "    4: \"deer\", \n",
    "    5: \"dog\", \n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "tensor_cifar10 = datasets.CIFAR10(\"data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "tensor_cifar10_val = datasets.CIFAR10(\"data\", train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
    "per_channel_means = imgs.view(3, -1).mean(dim=1)\n",
    "per_channel_std = imgs.view(3, -1).std(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_compose = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(per_channel_means, per_channel_std)\n",
    "])\n",
    "\n",
    "transformed_cifar10 = datasets.CIFAR10(\"data\", train=True, download=False, transform=transforms_compose)\n",
    "transformed_cifar10_val = datasets.CIFAR10(\"data\", train=False, download=False, transform=transforms_compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "new_class_names  = [class_names[i] for i in label_map]\n",
    "\n",
    "cifar2 = [(img, label_map[label]) for img, label in tensor_cifar10 if label in label_map]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in tensor_cifar10 if label in label_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "Uzupełnij poniższe komórki, aby wytrenować splotową sieć neuronową do zadania klasyfikacji binarnej.\n",
    "\n",
    "Przyjmij learning rate o wartości 0.01 i batch size 64. Trenuj przez 100 epok. Użyj optymaliatora SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcifar2_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 104.98927986621857\n",
      "Epoch 10, Training loss 69.9262126982212\n",
      "Epoch 20, Training loss 55.61783264577389\n",
      "Epoch 30, Training loss 51.27168031036854\n",
      "Epoch 40, Training loss 47.724389150738716\n",
      "Epoch 50, Training loss 45.0846983268857\n",
      "Epoch 60, Training loss 42.50561612844467\n",
      "Epoch 70, Training loss 40.05517219007015\n",
      "Epoch 80, Training loss 37.757189989089966\n",
      "Epoch 90, Training loss 35.95438706129789\n",
      "Epoch 100, Training loss 33.81690967828035\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "train_loader = DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walidacja modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Zaimplementuj funkcję `validate`, która zmierzy dokładność wytrenowanego modelu na dwóch zbiorach - uczącym i walidacyjnym.\n",
    "\n",
    "Porównaj wyniki z wynikami z laboratorium nr 5 (sieć gęsta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, train_loader, val_loader):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(imgs).to(device=device, dtype=torch.float32)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "        print(f\"{name} accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9126\n",
      "val accuracy: 0.9126\n"
     ]
    }
   ],
   "source": [
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zapis i odczyt modelu\n",
    "\n",
    "Wytrenowany model (zwłaszcza tak, którego trening trwa długo) warto zapisać, aby móc go użyć później.\n",
    "\n",
    "Typowo po każdej epoce treningu sprawdza się działanie modelu na zbiorze walidacyjnym (walidacja).\n",
    "Zapisu modelu (tzw. \"checkpoint\") typowo dokonuje się, jeśli wartość danej metryki (np. dokładność lub F1 na zbiorze walidacyjnym) jest lepsza niż najlepsza uzyskana dotychczas.\n",
    "\n",
    "PyTorch pozwala zapisać wagi (parametry) modelu z użyciem `torch.save` oraz tzw. `state_dict` modelu (https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html). Innym sposobem zapisu jest zapis całego modelu (z użyciem `pickle` \"pod spodem\"):\n",
    "https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html\n",
    "\n",
    "Następnie, do wczytania zapisanego modelu można użyć metody `load_state_dict` (oraz `torch.load`), jeśli zapisywaliśmy tylko `state_dict` lub tylko `torch.load`, jeśli zapisywaliśmy cały model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[ 0.0606, -0.2273, -0.0732],\n",
      "          [-0.0974, -0.2431, -0.1682],\n",
      "          [ 0.0541, -0.2974, -0.1833]],\n",
      "\n",
      "         [[-0.0880,  0.1890, -0.1167],\n",
      "          [ 0.1557, -0.0647, -0.1885],\n",
      "          [ 0.1671, -0.0270,  0.2232]],\n",
      "\n",
      "         [[ 0.0949,  0.2364,  0.1374],\n",
      "          [-0.1203,  0.2242,  0.1321],\n",
      "          [ 0.2106, -0.0045,  0.2103]]],\n",
      "\n",
      "\n",
      "        [[[-0.0051,  0.1984,  0.1453],\n",
      "          [-0.2703,  0.1930, -0.0929],\n",
      "          [ 0.0636,  0.2682,  0.2221]],\n",
      "\n",
      "         [[ 0.0434,  0.0888,  0.0075],\n",
      "          [-0.2599, -0.1527, -0.2117],\n",
      "          [ 0.1145, -0.0514,  0.0505]],\n",
      "\n",
      "         [[-0.0213,  0.0060, -0.1025],\n",
      "          [-0.1100, -0.1763, -0.1820],\n",
      "          [ 0.2429,  0.2124,  0.0694]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1587, -0.1347, -0.2053],\n",
      "          [ 0.0011, -0.0315, -0.1452],\n",
      "          [-0.1595,  0.1614,  0.0540]],\n",
      "\n",
      "         [[-0.1597,  0.0350,  0.1157],\n",
      "          [-0.2125, -0.1473, -0.2104],\n",
      "          [-0.1489, -0.0438, -0.0429]],\n",
      "\n",
      "         [[ 0.0213,  0.0526, -0.1939],\n",
      "          [-0.0005, -0.0328, -0.0208],\n",
      "          [ 0.1202, -0.0118,  0.0595]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0289,  0.1258,  0.2083],\n",
      "          [ 0.1759,  0.1704,  0.2561],\n",
      "          [ 0.2132, -0.0919,  0.2171]],\n",
      "\n",
      "         [[-0.0354, -0.1176, -0.1387],\n",
      "          [ 0.0372,  0.0184, -0.0632],\n",
      "          [-0.0270, -0.0924, -0.0242]],\n",
      "\n",
      "         [[-0.2442,  0.0391, -0.0383],\n",
      "          [ 0.1028,  0.0833, -0.1375],\n",
      "          [-0.1098, -0.2238, -0.2325]]],\n",
      "\n",
      "\n",
      "        [[[-0.1442, -0.2214,  0.1188],\n",
      "          [-0.1469, -0.0308, -0.2325],\n",
      "          [-0.2360, -0.1990, -0.1308]],\n",
      "\n",
      "         [[-0.1147, -0.0958, -0.0347],\n",
      "          [ 0.1614, -0.0306, -0.2066],\n",
      "          [ 0.1134, -0.1248, -0.0314]],\n",
      "\n",
      "         [[-0.0065, -0.0478, -0.0578],\n",
      "          [-0.0070, -0.0822,  0.1483],\n",
      "          [ 0.2410,  0.1149,  0.0958]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1742,  0.1204,  0.0600],\n",
      "          [-0.0950,  0.2214,  0.3543],\n",
      "          [-0.1620, -0.1188,  0.3524]],\n",
      "\n",
      "         [[-0.1440,  0.1517,  0.0615],\n",
      "          [-0.1443,  0.0708,  0.2671],\n",
      "          [-0.1461, -0.1612,  0.2405]],\n",
      "\n",
      "         [[-0.2074, -0.2196, -0.1329],\n",
      "          [-0.4067, -0.4199,  0.1815],\n",
      "          [-0.4060, -0.2894, -0.0593]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2007,  0.0464,  0.0962],\n",
      "          [-0.1977, -0.0145, -0.0421],\n",
      "          [ 0.1697,  0.1949,  0.1973]],\n",
      "\n",
      "         [[ 0.1407, -0.0533,  0.1502],\n",
      "          [-0.1611, -0.0879,  0.1803],\n",
      "          [-0.0611, -0.0696,  0.0946]],\n",
      "\n",
      "         [[ 0.1809,  0.1824, -0.1371],\n",
      "          [ 0.0289,  0.0439, -0.2098],\n",
      "          [-0.1146, -0.1935, -0.0208]]],\n",
      "\n",
      "\n",
      "        [[[-0.0760,  0.2479, -0.1709],\n",
      "          [ 0.1973,  0.2919,  0.1723],\n",
      "          [-0.2352,  0.0105,  0.0046]],\n",
      "\n",
      "         [[-0.0999,  0.1801,  0.1758],\n",
      "          [ 0.1803,  0.0228,  0.0811],\n",
      "          [-0.2144, -0.2497, -0.1028]],\n",
      "\n",
      "         [[ 0.1834, -0.1345, -0.0015],\n",
      "          [ 0.2300, -0.0630, -0.2085],\n",
      "          [-0.2701, -0.1392, -0.1220]]],\n",
      "\n",
      "\n",
      "        [[[-0.2643, -0.1799, -0.1880],\n",
      "          [ 0.1004,  0.1588, -0.1313],\n",
      "          [ 0.3496,  0.3113,  0.3501]],\n",
      "\n",
      "         [[-0.2655,  0.0703, -0.0785],\n",
      "          [-0.1655,  0.0642, -0.1536],\n",
      "          [ 0.2069,  0.2144,  0.2630]],\n",
      "\n",
      "         [[-0.2804, -0.0515,  0.0879],\n",
      "          [ 0.0075, -0.1356, -0.1307],\n",
      "          [-0.0627, -0.1998,  0.0911]]],\n",
      "\n",
      "\n",
      "        [[[-0.3128, -0.2639, -0.0809],\n",
      "          [ 0.0333,  0.2334,  0.2599],\n",
      "          [ 0.2057,  0.1984,  0.1134]],\n",
      "\n",
      "         [[-0.1633, -0.2032, -0.1684],\n",
      "          [-0.0072, -0.0057,  0.0919],\n",
      "          [-0.1364, -0.1818, -0.1083]],\n",
      "\n",
      "         [[-0.2618, -0.0104, -0.1918],\n",
      "          [-0.0785,  0.1767,  0.2723],\n",
      "          [ 0.2500,  0.2586, -0.0475]]],\n",
      "\n",
      "\n",
      "        [[[-0.2633, -0.1682, -0.1553],\n",
      "          [-0.4249, -0.0249, -0.2955],\n",
      "          [ 0.0267,  0.0085, -0.1656]],\n",
      "\n",
      "         [[-0.0346,  0.1775, -0.1998],\n",
      "          [-0.1773, -0.2317, -0.2571],\n",
      "          [ 0.0182, -0.0582, -0.2089]],\n",
      "\n",
      "         [[ 0.4420,  0.5001,  0.3959],\n",
      "          [ 0.2489,  0.3899,  0.2606],\n",
      "          [ 0.2251,  0.1850,  0.3126]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0421, -0.0835,  0.1045],\n",
      "          [-0.0038,  0.0331, -0.2372],\n",
      "          [-0.1260, -0.2655, -0.3526]],\n",
      "\n",
      "         [[ 0.3995,  0.1805,  0.3446],\n",
      "          [ 0.1646,  0.1973,  0.1852],\n",
      "          [-0.2141,  0.0972, -0.2818]],\n",
      "\n",
      "         [[-0.0836,  0.2176,  0.1182],\n",
      "          [-0.0690,  0.0921, -0.0273],\n",
      "          [-0.1243, -0.2551, -0.2908]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2280,  0.2150,  0.3905],\n",
      "          [ 0.0834, -0.1980,  0.0169],\n",
      "          [ 0.2452,  0.0429,  0.0891]],\n",
      "\n",
      "         [[ 0.0278, -0.2649,  0.1343],\n",
      "          [-0.0376, -0.2286,  0.1144],\n",
      "          [ 0.1240, -0.1769, -0.2345]],\n",
      "\n",
      "         [[-0.3238, -0.2582,  0.0574],\n",
      "          [-0.0313, -0.2906, -0.1870],\n",
      "          [ 0.2386, -0.1057, -0.2607]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0416,  0.1567, -0.0802],\n",
      "          [ 0.2836,  0.2045,  0.0368],\n",
      "          [ 0.1295,  0.1588,  0.1126]],\n",
      "\n",
      "         [[-0.2789, -0.3556,  0.0059],\n",
      "          [-0.1247, -0.1171, -0.3046],\n",
      "          [-0.2435, -0.0727, -0.0041]],\n",
      "\n",
      "         [[ 0.0853, -0.0682, -0.1114],\n",
      "          [ 0.0321,  0.1251, -0.0130],\n",
      "          [ 0.2217,  0.1452,  0.2263]]],\n",
      "\n",
      "\n",
      "        [[[-0.1690, -0.0837, -0.0664],\n",
      "          [ 0.1480,  0.1587,  0.1759],\n",
      "          [-0.1562,  0.1464,  0.2045]],\n",
      "\n",
      "         [[-0.0387,  0.2226, -0.0752],\n",
      "          [ 0.2668,  0.0718, -0.0872],\n",
      "          [-0.1305, -0.2269, -0.0305]],\n",
      "\n",
      "         [[ 0.0621,  0.2269, -0.2026],\n",
      "          [ 0.2340, -0.0278, -0.2372],\n",
      "          [ 0.1438, -0.1770, -0.2278]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0480, -0.1923, -0.1363],\n",
      "          [-0.1095,  0.1837, -0.1413],\n",
      "          [ 0.0640, -0.0118, -0.0838]],\n",
      "\n",
      "         [[-0.1633, -0.0575,  0.0591],\n",
      "          [-0.0408,  0.1919,  0.0939],\n",
      "          [ 0.1827,  0.2100, -0.0189]],\n",
      "\n",
      "         [[-0.2139,  0.0340, -0.0232],\n",
      "          [ 0.1669, -0.0190,  0.2131],\n",
      "          [ 0.0757,  0.1185, -0.0747]]]])), ('conv1.bias', tensor([ 0.0631, -0.2026,  0.1801, -0.1255, -0.0149,  0.2688, -0.2352, -0.0387,\n",
      "        -0.0205, -0.2143, -0.1244,  0.0345,  0.1359, -0.1568, -0.1156, -0.0191])), ('conv2.weight', tensor([[[[-0.0017, -0.1467, -0.1136],\n",
      "          [-0.0532, -0.1608, -0.1251],\n",
      "          [ 0.0006, -0.1230, -0.0922]],\n",
      "\n",
      "         [[-0.0301,  0.0647,  0.0558],\n",
      "          [ 0.0381,  0.0186,  0.0431],\n",
      "          [ 0.0271,  0.0657,  0.0403]],\n",
      "\n",
      "         [[-0.0591,  0.0403, -0.0264],\n",
      "          [-0.0924, -0.0787, -0.0315],\n",
      "          [-0.0044,  0.0452, -0.0174]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0951,  0.1548,  0.1398],\n",
      "          [ 0.1267,  0.1143,  0.1798],\n",
      "          [ 0.1377,  0.1068,  0.0934]],\n",
      "\n",
      "         [[ 0.1493,  0.0023,  0.0008],\n",
      "          [ 0.0239, -0.0439,  0.0574],\n",
      "          [ 0.0979, -0.0447, -0.0400]],\n",
      "\n",
      "         [[-0.0004, -0.0661, -0.0867],\n",
      "          [-0.0783, -0.0439,  0.0066],\n",
      "          [-0.0482, -0.1309, -0.0590]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0022, -0.0590, -0.0452],\n",
      "          [-0.1050, -0.0717, -0.0162],\n",
      "          [-0.0612, -0.0912, -0.0153]],\n",
      "\n",
      "         [[-0.0056,  0.0270, -0.0167],\n",
      "          [-0.0047, -0.0788, -0.0523],\n",
      "          [-0.0076, -0.0130, -0.0019]],\n",
      "\n",
      "         [[ 0.0019,  0.0933, -0.0351],\n",
      "          [-0.0326, -0.0286, -0.0822],\n",
      "          [-0.0633, -0.0108,  0.0632]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0023,  0.0037,  0.0506],\n",
      "          [ 0.0014, -0.0207, -0.0014],\n",
      "          [-0.1113, -0.0431,  0.0406]],\n",
      "\n",
      "         [[ 0.0021, -0.0369, -0.1078],\n",
      "          [-0.0090,  0.1096,  0.0524],\n",
      "          [ 0.1138, -0.0029,  0.0086]],\n",
      "\n",
      "         [[-0.0775,  0.0382,  0.0512],\n",
      "          [ 0.0253,  0.0286,  0.0097],\n",
      "          [-0.0057, -0.0478, -0.1084]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0207,  0.1365,  0.1620],\n",
      "          [ 0.1359,  0.1649,  0.1251],\n",
      "          [-0.0009, -0.0700, -0.0330]],\n",
      "\n",
      "         [[ 0.2204,  0.1635,  0.2479],\n",
      "          [ 0.0222,  0.1072,  0.1277],\n",
      "          [ 0.1137,  0.1360,  0.1289]],\n",
      "\n",
      "         [[ 0.0367,  0.0222,  0.0315],\n",
      "          [ 0.0711,  0.1420,  0.1468],\n",
      "          [ 0.0716, -0.0105,  0.0662]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0489,  0.1441,  0.0854],\n",
      "          [ 0.0420,  0.0345,  0.0286],\n",
      "          [ 0.0719,  0.1206, -0.0027]],\n",
      "\n",
      "         [[ 0.0731,  0.0955, -0.0121],\n",
      "          [ 0.1386,  0.1842,  0.1365],\n",
      "          [ 0.0812,  0.0073,  0.0127]],\n",
      "\n",
      "         [[ 0.0873,  0.0960,  0.0291],\n",
      "          [ 0.1053,  0.0396,  0.0746],\n",
      "          [ 0.0131, -0.0091,  0.0173]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0120, -0.0218, -0.1279],\n",
      "          [ 0.1062,  0.0905,  0.0096],\n",
      "          [ 0.0227,  0.1017, -0.0471]],\n",
      "\n",
      "         [[-0.0446, -0.0645, -0.0173],\n",
      "          [-0.0102, -0.0443, -0.0420],\n",
      "          [ 0.0947, -0.0387,  0.0908]],\n",
      "\n",
      "         [[-0.0175,  0.1296, -0.0177],\n",
      "          [ 0.0281, -0.0413,  0.0297],\n",
      "          [-0.0647, -0.0566, -0.0980]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0635,  0.0133,  0.0392],\n",
      "          [ 0.1450,  0.1678,  0.0263],\n",
      "          [ 0.1122,  0.1440, -0.0114]],\n",
      "\n",
      "         [[ 0.0966, -0.0483,  0.0449],\n",
      "          [ 0.0114, -0.0577, -0.0578],\n",
      "          [-0.0288,  0.0260, -0.0573]],\n",
      "\n",
      "         [[-0.0924,  0.0192, -0.1011],\n",
      "          [ 0.0826,  0.0349,  0.0331],\n",
      "          [-0.0907, -0.0080, -0.0218]]],\n",
      "\n",
      "\n",
      "        [[[-0.0296, -0.0649, -0.0825],\n",
      "          [ 0.0501, -0.0282, -0.0239],\n",
      "          [ 0.0553, -0.0888,  0.0218]],\n",
      "\n",
      "         [[-0.0996,  0.0283, -0.0504],\n",
      "          [ 0.0230, -0.0036, -0.0173],\n",
      "          [-0.0500,  0.0487,  0.0546]],\n",
      "\n",
      "         [[ 0.1417,  0.2022,  0.1815],\n",
      "          [ 0.1395,  0.1009,  0.0726],\n",
      "          [ 0.1061,  0.0112,  0.1074]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0103,  0.0646, -0.0326],\n",
      "          [-0.0732,  0.0792, -0.0246],\n",
      "          [ 0.0124, -0.0455,  0.0123]],\n",
      "\n",
      "         [[ 0.0524,  0.0757, -0.0724],\n",
      "          [-0.0725, -0.0450, -0.0581],\n",
      "          [-0.0593, -0.0455, -0.1103]],\n",
      "\n",
      "         [[-0.1123,  0.0069,  0.0369],\n",
      "          [-0.0803,  0.0150,  0.0088],\n",
      "          [ 0.0595,  0.0074, -0.0725]]],\n",
      "\n",
      "\n",
      "        [[[-0.1371, -0.1110,  0.0729],\n",
      "          [-0.0071, -0.1050,  0.0029],\n",
      "          [-0.1110, -0.0346,  0.0550]],\n",
      "\n",
      "         [[-0.0526, -0.0515, -0.0443],\n",
      "          [ 0.1390, -0.0723, -0.1066],\n",
      "          [ 0.0523, -0.0787, -0.1145]],\n",
      "\n",
      "         [[ 0.0615,  0.0701, -0.0327],\n",
      "          [ 0.0337, -0.0371, -0.0387],\n",
      "          [ 0.0096,  0.0820, -0.0576]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0252, -0.0707,  0.0241],\n",
      "          [-0.0109, -0.1136,  0.0390],\n",
      "          [-0.0156, -0.0654, -0.1187]],\n",
      "\n",
      "         [[ 0.1425,  0.0052, -0.1151],\n",
      "          [ 0.0468,  0.0135, -0.1023],\n",
      "          [ 0.1932, -0.0213, -0.0556]],\n",
      "\n",
      "         [[-0.0684, -0.1054,  0.0418],\n",
      "          [ 0.0175, -0.0945,  0.0413],\n",
      "          [ 0.0850, -0.0069,  0.0013]]]])), ('conv2.bias', tensor([ 0.0352, -0.0181, -0.0710, -0.1651,  0.0328,  0.0903,  0.1036,  0.0492])), ('fc1.weight', tensor([[-0.0172,  0.0426, -0.0275,  ...,  0.0324,  0.0304,  0.0076],\n",
      "        [ 0.0015,  0.0545, -0.0179,  ...,  0.0765, -0.0045, -0.0215],\n",
      "        [ 0.0214, -0.0453,  0.0398,  ...,  0.0029,  0.0302,  0.0337],\n",
      "        ...,\n",
      "        [ 0.0346, -0.0048, -0.0424,  ...,  0.0121,  0.0194,  0.0039],\n",
      "        [-0.0481,  0.0224,  0.0152,  ...,  0.0501, -0.0095, -0.0287],\n",
      "        [-0.0150,  0.0004, -0.0317,  ...,  0.0073, -0.0374, -0.0236]])), ('fc1.bias', tensor([-0.0195, -0.0367,  0.0506,  0.0528,  0.0894,  0.0009,  0.0133, -0.0297,\n",
      "        -0.0699, -0.0448, -0.0127, -0.0032, -0.0141,  0.0012,  0.0660, -0.0166,\n",
      "         0.0480, -0.0573,  0.0265, -0.0748,  0.0573, -0.0275,  0.0327, -0.0325,\n",
      "        -0.0180, -0.0321, -0.0486, -0.0169,  0.0079, -0.0508, -0.0355, -0.0050])), ('fc2.weight', tensor([[ 0.3281, -0.3454,  0.2287,  0.1527,  0.4030,  0.3809,  0.6199, -0.1688,\n",
      "         -0.1431, -0.0503, -0.4156, -0.1765,  0.1134, -0.3824, -0.4963, -0.2178,\n",
      "          0.2667, -0.4846, -0.2061, -0.4368,  0.5427,  0.5829,  0.1000, -0.1296,\n",
      "          0.2322,  0.0520, -0.0254, -0.1064,  0.2724, -0.4005, -0.2608, -0.2620],\n",
      "        [-0.0069,  0.4387, -0.2302, -0.3049, -0.4344, -0.2680, -0.5907,  0.1991,\n",
      "          0.1864,  0.0688,  0.3785,  0.2229,  0.1524,  0.4393,  0.5322, -0.0515,\n",
      "         -0.4143,  0.4918,  0.3461,  0.4805, -0.4785, -0.5982, -0.2937, -0.1792,\n",
      "          0.0386,  0.2115,  0.2910,  0.2812, -0.4376,  0.4779,  0.1930,  0.0310]])), ('fc2.bias', tensor([ 0.0370, -0.1056]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"data/birds_vs_airplanes.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net()\n",
    "loaded_model.load_state_dict(torch.load(\"data/birds_vs_airplanes.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening na GPU (opcjonalnie)\n",
    "\n",
    "Aby przyspieszyć trening (zwłaszcza w przypadku głębokich modeli i dużych zbiorów danych), powszechnie stosuje się karty graficzne (GPU). Jeśli mamy dostęp do maszyny z kartą graficzną (najlepiej od NVIDIA, obsługującą CUDA), możemy łatwo \"przenieść\" trening na GPU.\n",
    "\n",
    "W tym celu należy przenieść zarówno dane, jak i model, na kartę graficzną, używając metody `.to` (tensora i `nn.Module`) na zdefiniowane urządzenie (patrz poniżej)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Zmodyfikuj napisaną wyżej pętlę treningową oraz inicjalizację modelu, przenosząc odpowiednio dane (obrazki i etykiety) oraz model na `device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 0.6840003303661468\n",
      "Epoch 10, Training loss 0.46557374440940325\n",
      "Epoch 20, Training loss 0.35944290231367587\n",
      "Epoch 30, Training loss 0.3267110019542609\n",
      "Epoch 40, Training loss 0.2992238887366216\n",
      "Epoch 50, Training loss 0.2801251609803765\n",
      "Epoch 60, Training loss 0.26367165366555473\n",
      "Epoch 70, Training loss 0.25008184714302134\n",
      "Epoch 80, Training loss 0.2378904631563053\n",
      "Epoch 90, Training loss 0.22749763893283856\n",
      "Epoch 100, Training loss 0.21602441434552716\n"
     ]
    }
   ],
   "source": [
    "model_gpu = Net().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_gpu.parameters(), lr=0.01)\n",
    "train_loader = DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "training_loop(n_epochs=100, optimizer=optimizer, model=model_gpu, loss_fn=loss_fn, train_loader=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rozbudowa modelu\n",
    "\n",
    "Możemy \"powiększyć\" model \"na szerokość\" (dodać więcej filtrów) lub \"na głębokość\" (dodać więcej warstw)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Zmodyfikuj klasę `Net` i stwórz kolejno:\n",
    "- `NetWidth` - 2x więcej filtrów w warstwach splotowych (niech liczba filtrów będzie argumentem konstruktora)\n",
    "- `NetDepth` - dodatkowa warstwa splotowa `conv3`\n",
    "\n",
    "Pamiętaj o zmodyfikowaniu metody `forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1*2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1*2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans*2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1, n_chans1, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularyzacja L2 / weight decay\n",
    "\n",
    "Regularyzację L2 można zaimplementować samemu (jak niżej). \n",
    "\n",
    "Jest ona jednak wbudowana w `torch.optim`, np. https://pytorch.org/docs/stable/optim.html#torch.optim.SGD, gdzie wystarczy podać wartość `weight_decay` tworząc optymalizator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "W poniższej pętli treningowej dopisz fragment realizujący regularyzację L2 dla lambda = 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "                        \n",
    "            l2_lambda = 0.001\n",
    "            for param in model.parameters():\n",
    "                loss += l2_lambda * torch.norm(param)**2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 109.95175862312317\n",
      "Epoch 10, Training loss 71.605403393507\n",
      "Epoch 20, Training loss 59.85934379696846\n",
      "Epoch 30, Training loss 56.33640371263027\n",
      "Epoch 40, Training loss 53.65350362658501\n",
      "Epoch 50, Training loss 51.90621429681778\n",
      "Epoch 60, Training loss 50.32947988808155\n",
      "Epoch 70, Training loss 48.58898697793484\n",
      "Epoch 80, Training loss 47.131951063871384\n",
      "Epoch 90, Training loss 45.31257829070091\n",
      "Epoch 100, Training loss 43.84178914129734\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Wywołaj \"zwykłą\" pętlę treningową, tym razem podająć `weight_decay` optyamlizatora równe 0.001. Zaobserwuj wpływ na funkcję straty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 106.9676085114479\n",
      "Epoch 10, Training loss 69.2920361161232\n",
      "Epoch 20, Training loss 56.69583120942116\n",
      "Epoch 30, Training loss 53.437835335731506\n",
      "Epoch 40, Training loss 50.53909331560135\n",
      "Epoch 50, Training loss 48.41050708293915\n",
      "Epoch 60, Training loss 45.966328859329224\n",
      "Epoch 70, Training loss 43.805709198117256\n",
      "Epoch 80, Training loss 41.343967974185944\n",
      "Epoch 90, Training loss 39.20707976818085\n",
      "Epoch 100, Training loss 37.02289388328791\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, weight_decay=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def training_loop_l2reg_weight_decay(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train}\")\n",
    "\n",
    "training_loop_l2reg_weight_decay(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "Dołóż warstwy `nn.Dropout2d` po warstwach splotowych (po max poolingu) do sieci `Net` i stwórz w ten sposob `NetDropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.dropout(out)\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = self.dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie\n",
    "\n",
    "Dodaj warstwy `BatchNorm2d` po warstwach splotowych `Net` tworząc w ten sposób `NetBatchNorm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()        \n",
    "        self.n_chans = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(n_chans1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.bn1(self.conv1(x))), 2)\n",
    "        out = nn.functional.max_pool2d(torch.relu(self.bn2(self.conv2(out))), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ćwiczenie (zadanie domowe)\n",
    "\n",
    "Porównaj wyniki uzyskane przez każdą z zaimplementowanych klas na zbiorze uczącym i walidacyjnym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 109.94240826368332\n",
      "Epoch 10, Training loss 72.01702234148979\n",
      "Epoch 20, Training loss 60.403788566589355\n",
      "Epoch 30, Training loss 57.19509010016918\n",
      "Epoch 40, Training loss 54.897967368364334\n",
      "Epoch 50, Training loss 52.99198879301548\n",
      "Epoch 60, Training loss 51.31756116449833\n",
      "Epoch 70, Training loss 49.67265437543392\n",
      "Epoch 80, Training loss 48.047274589538574\n",
      "Epoch 90, Training loss 46.43214137852192\n",
      "Epoch 100, Training loss 44.91967725753784\n",
      "Walidacja modelu: Net\n",
      "train accuracy: 0.8862\n",
      "val accuracy: 0.8862\n",
      "Epoch 1, Training loss 113.74421441555023\n",
      "Epoch 10, Training loss 77.03097426891327\n",
      "Epoch 20, Training loss 61.95955556631088\n",
      "Epoch 30, Training loss 56.88470958173275\n",
      "Epoch 40, Training loss 53.255750462412834\n",
      "Epoch 50, Training loss 49.844123497605324\n",
      "Epoch 60, Training loss 46.520743668079376\n",
      "Epoch 70, Training loss 43.49854123592377\n",
      "Epoch 80, Training loss 40.889246091246605\n",
      "Epoch 90, Training loss 38.62727456539869\n",
      "Epoch 100, Training loss 36.575046576559544\n",
      "Walidacja modelu: NetWidth\n",
      "train accuracy: 0.9104\n",
      "val accuracy: 0.9104\n",
      "Epoch 1, Training loss 115.66970908641815\n",
      "Epoch 10, Training loss 89.44026148319244\n",
      "Epoch 20, Training loss 80.30813398957253\n",
      "Epoch 30, Training loss 66.02487251162529\n",
      "Epoch 40, Training loss 59.29212233424187\n",
      "Epoch 50, Training loss 56.114913910627365\n",
      "Epoch 60, Training loss 53.822655349969864\n",
      "Epoch 70, Training loss 51.809637665748596\n",
      "Epoch 80, Training loss 49.69972664117813\n",
      "Epoch 90, Training loss 47.45785450935364\n",
      "Epoch 100, Training loss 45.176643669605255\n",
      "Walidacja modelu: NetDepth\n",
      "train accuracy: 0.8818\n",
      "val accuracy: 0.8818\n",
      "Epoch 1, Training loss 114.40293717384338\n",
      "Epoch 10, Training loss 89.28926768898964\n",
      "Epoch 20, Training loss 73.9742941558361\n",
      "Epoch 30, Training loss 67.61144256591797\n",
      "Epoch 40, Training loss 64.99576985836029\n",
      "Epoch 50, Training loss 62.82135696709156\n",
      "Epoch 60, Training loss 58.97925269603729\n",
      "Epoch 70, Training loss 58.002614706754684\n",
      "Epoch 80, Training loss 56.75195640325546\n",
      "Epoch 90, Training loss 55.082984045147896\n",
      "Epoch 100, Training loss 53.20528410375118\n",
      "Walidacja modelu: NetDropout\n",
      "train accuracy: 0.8769\n",
      "val accuracy: 0.8769\n",
      "Epoch 1, Training loss 85.99784445762634\n",
      "Epoch 10, Training loss 50.14874538779259\n",
      "Epoch 20, Training loss 36.702735647559166\n",
      "Epoch 30, Training loss 26.67964158952236\n",
      "Epoch 40, Training loss 26.454832017421722\n",
      "Epoch 50, Training loss 17.485146895051003\n",
      "Epoch 60, Training loss 15.931509107351303\n",
      "Epoch 70, Training loss 14.94524722546339\n",
      "Epoch 80, Training loss 14.163668431341648\n",
      "Epoch 90, Training loss 13.500843465328217\n",
      "Epoch 100, Training loss 12.90563452243805\n",
      "Walidacja modelu: NetBatchNorm\n",
      "train accuracy: 0.9983\n",
      "val accuracy: 0.9983\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")\n",
    "print(f\"Walidacja modelu: {model.__class__.__name__}\")\n",
    "validate(model, train_loader, val_loader)\n",
    "\n",
    "\n",
    "model_NetWidth= NetWidth().to(device)\n",
    "optimizer = torch.optim.SGD(model_NetWidth.parameters(), lr=1e-2)\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model_NetWidth,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")\n",
    "print(f\"Walidacja modelu: {model_NetWidth.__class__.__name__}\")\n",
    "validate(model_NetWidth, train_loader, val_loader)\n",
    "\n",
    "\n",
    "model_NetDepth = NetDepth().to(device)\n",
    "optimizer = torch.optim.SGD(model_NetDepth.parameters(), lr=1e-2)\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model_NetDepth,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")\n",
    "print(f\"Walidacja modelu: {model_NetDepth.__class__.__name__}\")\n",
    "validate(model_NetDepth, train_loader, val_loader)\n",
    "\n",
    "\n",
    "model_NetDropout = NetDropout().to(device)\n",
    "optimizer = torch.optim.SGD(model_NetDropout.parameters(), lr=1e-2)\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model_NetDropout,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")\n",
    "print(f\"Walidacja modelu: {model_NetDropout.__class__.__name__}\")\n",
    "validate(model_NetDropout, train_loader, val_loader)\n",
    "\n",
    "model_NetBatchNorm = NetBatchNorm().to(device)\n",
    "optimizer = torch.optim.SGD(model_NetBatchNorm.parameters(), lr=1e-2)\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model_NetBatchNorm,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")\n",
    "print(f\"Walidacja modelu: {model_NetBatchNorm.__class__.__name__}\")\n",
    "validate(model_NetBatchNorm, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x512 and 2048x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model_NetDepth \u001b[38;5;241m=\u001b[39m NetDepth()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model_NetDepth\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtraining_loop_l2reg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_NetDepth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWalidacja modelu: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_NetDepth\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m validate(model_NetDepth, train_loader, val_loader)\n",
      "Cell \u001b[1;32mIn[43], line 7\u001b[0m, in \u001b[0;36mtraining_loop_l2reg\u001b[1;34m(n_epochs, optimizer, model, loss_fn, train_loader)\u001b[0m\n\u001b[0;32m      5\u001b[0m imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m----> 7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m     10\u001b[0m l2_lambda \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Cezary\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Cezary\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[49], line 16\u001b[0m, in \u001b[0;36mNetDepth.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m out \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmax_pool2d(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     15\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_chans)\n\u001b[1;32m---> 16\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     17\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(out)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Cezary\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Cezary\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Cezary\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x512 and 2048x32)"
     ]
    }
   ],
   "source": [
    "model_NetDepth = NetDepth().to(device)\n",
    "optimizer = torch.optim.SGD(model_NetDepth.parameters(), lr=1e-2)\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model_NetDepth,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")\n",
    "print(f\"Walidacja modelu: {model_NetDepth.__class__.__name__}\")\n",
    "validate(model_NetDepth, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
